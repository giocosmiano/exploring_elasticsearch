# how to load `uszips.csv` to elastic via logstash
# $ cd ~/Documents/_projects/exploring_elasticsearch/config/logstash/resources
# $ cat uszips.csv | logstash -f uszips.conf
input {
    stdin { }
}

filter {
    csv {
        separator => ","
        columns => [ "zip", "lat", "lng", "city", "state_id", "state_name", "zcta", "parent_zcta", "population", "density", "county_fips", "county_name", "county_weights", "county_names_all", "county_fips_all", "imprecise", "military", "timezone" ]
    }

    uuid {
        target => "uuid"
        overwrite => true
    }

# Don't convert `county_weights` to json as it'll generate separate field for each property
#     json {
#         source => "county_weights"
#     }

    mutate {
        convert => { "zip" => "integer" }
        convert => { "latitude" => "float" }
        convert => { "longitude" => "float" }
        convert => { "population" => "integer" }
        convert => { "density" => "float" }
        convert => { "county_fips" => "integer" }
        convert => { "county_fips_all" => "integer" }
        split => [ "county_fips_all", "," ]
        rename => [ "lat", "latitude" ]
        rename => [ "lng", "longitude" ]
        rename => [ "state_id", "state_code" ]
        add_field => [ "geo_location", "%{latitude},%{longitude}" ]
        remove_field => [ "zcta", "parent_zcta", "imprecise", "military", "message" ]
    }
}

output {
#    elasticsearch {
#        index => "uszips_index"
#        hosts => [ "http://localhost:9200" ]
#        document_type => "doc"
#        document_id => "%{uuid}"
#        manage_template => true
#        template_overwrite => true
#        template => "/home/lisa/Documents/_projects/exploring_elasticsearch/config/logstash/resources/uszips_index_template.json"
#        template_name => "uszips_index"
#    }
    stdout { codec => rubydebug }
}